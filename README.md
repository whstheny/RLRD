# RLRD:Reinforced Multi-Teacher Knowledge Distillation for Unsupervised Sentence Representation

we propose a novel multi-teacher ranking distillation approach based on
reinforcement learning, which dynamically adjusts the weights of teacher
models to further enhance the sentence representation capabilities of the
student model. Experimental results demonstrate that the proposed approach surpasses the current state-of-the-art methods in most Semantic
Textual Similarity (STS) tasks, showcasing its significant capability in
unsupervised sentence representation learning.

## Citations

Please cite RLRD if you use this repository or find it useful in some way. 

```bibtex
@inproceedings{wang2024reinforced,  
  title={Reinforced Multi-Teacher Knowledge Distillation for Unsupervised Sentence Representation},  
  author={Wang, Xintao and Jin, Rize and Qi, Shibo},  
  booktitle={The 33rd International Conference on Artificial Neural Networks.},  
  year={2024}  
}  
